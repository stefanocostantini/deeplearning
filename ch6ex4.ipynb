{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib64/python3.4/importlib/_bootstrap.py:321: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1D convnets have been used successfully for text classificaton and time series forecasting. They are less computational expensive than RNN. In Keras they can be implemented with the `Conv1D` layer, which takes as input a 3D tensor with shape `(shape, time, features)` and returns similarly shaped 3D tensors. The convolution window is a 1D window on the temporal axis: axis 1 in the input tensor.\n",
    "We build a simple two-layer 1D convnet and apply it to the IMDB data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 500)\n",
      "x_test shape: (25000, 500)\n"
     ]
    }
   ],
   "source": [
    "# Let's first load and prepare the data\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features = 10000\n",
    "max_len = 500\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 128)          1280000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,315,937\n",
      "Trainable params: 1,315,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 7s 352us/step - loss: 0.8337 - acc: 0.5095 - val_loss: 0.6874 - val_acc: 0.5656\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.6699 - acc: 0.6385 - val_loss: 0.6641 - val_acc: 0.6574\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.6235 - acc: 0.7534 - val_loss: 0.6080 - val_acc: 0.7430\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.5258 - acc: 0.8086 - val_loss: 0.4825 - val_acc: 0.8056\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 3s 172us/step - loss: 0.4130 - acc: 0.8477 - val_loss: 0.4335 - val_acc: 0.8284\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 3s 170us/step - loss: 0.3487 - acc: 0.8666 - val_loss: 0.4145 - val_acc: 0.8340\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.3089 - acc: 0.8646 - val_loss: 0.4321 - val_acc: 0.8246\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.2797 - acc: 0.8474 - val_loss: 0.4269 - val_acc: 0.8038\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.2523 - acc: 0.8328 - val_loss: 0.4460 - val_acc: 0.7864\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 3s 171us/step - loss: 0.2299 - acc: 0.8107 - val_loss: 0.4987 - val_acc: 0.7510\n"
     ]
    }
   ],
   "source": [
    "# Then let's set up and train the model\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=max_len))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=10,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation accuracy is somewhat less than that of the LSTM, but runtime is faster on both CPU and GPU (the exact increase in speed will vary greatly depending on your exact configuration)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining CNN and RNN for longer sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with 1D convnets is that they process input patches independently, so they aren't sensitive to the order of the timesteps, unlike RNN. Of course, to recognize longer-term patterns, you can stack many convo- lution layers and pooling layers, resulting in upper layers that will see long chunks of the original inputs—but that’s still a fairly weak way to induce order sensitivity.\n",
    "\n",
    "We can see that if we apply a 1D convnet to the temperature forecasting problem, the `MAE` doesn't drop below 0.40 (see example page 229), which is even worse than the non-ML baseline. Because more recent data points should be interpreted differently from older data points in the case of this specific forecasting problem, the convnet fails at producing meaningful results. \n",
    "\n",
    "This limitation of convnets isn’t an issue with the IMDB data, because patterns of keywords associated with a positive or negative sentiment are informative independently of where they’re found in the input sentences.\n",
    "\n",
    "One strategy to combine the speed and lightness of convnets with the order-sensitivity of RNNs is to use a 1D convnet as a preprocessing step before an RNN. Let's try this technique in the temperature forecasting problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first reload the data\n",
    "import os\n",
    "\n",
    "data_dir = \"/home/ec2-user/datasets/jena_climate/\"\n",
    "fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv')\n",
    "\n",
    "f = open(fname)\n",
    "data = f.read()\n",
    "f.close()\n",
    "\n",
    "lines = data.split('\\n')\n",
    "header = lines[0].split(',')\n",
    "lines = lines[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import into Numpy\n",
    "import numpy as np\n",
    "float_data = np.zeros((len(lines), len(header) - 1))\n",
    "for i, line in enumerate(lines):\n",
    "    values = [float(x) for x in line.split(',')[1:]]\n",
    "    float_data[i, :] = values\n",
    "# As a first step, we normalise the data, so that all variables\n",
    "# are of a similar magnitude\n",
    "mean = float_data[:200000].mean(axis=0)\n",
    "float_data -= mean\n",
    "std = float_data[:200000].std(axis=0)\n",
    "float_data /= std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then we need to write a generator that takes the current array of float data and yields\n",
    "# batches of data from the recent past, along with a target temperature in the future. It yields a tuple \n",
    "# (samples, targets), where samples is one batch of input data and targets is the corresponding array \n",
    "# of target temperatures.\n",
    "def generator(data, lookback, delay, min_index, max_index,\n",
    "             shuffle=False, batch_size=128, step=6):\n",
    "    if max_index is None:\n",
    "        max_index = len(data) - delay - 1\n",
    "    i = min_index + lookback\n",
    "    while 1:\n",
    "        if shuffle:\n",
    "            rows = np.random.randint(\n",
    "                min_index + lookback, max_index, size=batch_size)\n",
    "        else:\n",
    "            if i + batch_size >= max_index:\n",
    "                i = min_index + lookback\n",
    "            rows = np.arange(i, min(i + batch_size, max_index))\n",
    "            i += len(rows)\n",
    "            \n",
    "        samples = np.zeros((len(rows), lookback // step, data.shape[-1]))\n",
    "        targets = np.zeros((len(rows), ))\n",
    "        for j, row in enumerate(rows):\n",
    "            indices = range(rows[j] - lookback, rows[j], step)\n",
    "            samples[j] = data[indices]\n",
    "            targets[j] = data[rows[j] + delay][1]\n",
    "        yield samples, targets    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step is now equal to 3, which means we now look at 1 point every 30 minutes. \n",
    "# This means we're going to look at twice the data. Lookback and and delay stay the same.\n",
    "step = 3\n",
    "lookback = 720\n",
    "delay = 144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator(float_data,\n",
    "                      lookback=lookback,\n",
    "                      delay=delay,\n",
    "                      min_index=0,\n",
    "                      max_index=200000,\n",
    "                      shuffle=True,\n",
    "                      step=step)\n",
    "val_gen = generator(float_data,\n",
    "                    lookback=lookback,\n",
    "                    delay=delay,\n",
    "                    min_index=200001,\n",
    "                    max_index=300000,\n",
    "                    step=step)\n",
    "test_gen = generator(float_data,\n",
    "                     lookback=lookback,\n",
    "                     delay=delay,\n",
    "                     min_index=300001,\n",
    "                     max_index=None,\n",
    "                     step=step)\n",
    "val_steps = (300000 - 200001 - lookback) // 128\n",
    "test_steps = (len(float_data) - 300001 - lookback) // 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_3 (Conv1D)            (None, None, 32)          2272      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, None, 32)          5152      \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 32)                6240      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 13,697\n",
      "Trainable params: 13,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# This is the model, starting with two Conv1D layers and following up with a GRU layer.\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Conv1D(32, 5, activation='relu',\n",
    "                       input_shape=(None, float_data.shape[-1])))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "model.add(layers.GRU(32, dropout=0.1, recurrent_dropout=0.5))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "500/500 [==============================] - 74s 148ms/step - loss: 0.3300 - val_loss: 0.2818\n",
      "Epoch 2/20\n",
      "500/500 [==============================] - 73s 145ms/step - loss: 0.3025 - val_loss: 0.3006\n",
      "Epoch 3/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2909 - val_loss: 0.2743\n",
      "Epoch 4/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2818 - val_loss: 0.2698\n",
      "Epoch 5/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2744 - val_loss: 0.2787\n",
      "Epoch 6/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2682 - val_loss: 0.2790\n",
      "Epoch 7/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2596 - val_loss: 0.2795\n",
      "Epoch 8/20\n",
      "500/500 [==============================] - 73s 145ms/step - loss: 0.2565 - val_loss: 0.2814\n",
      "Epoch 9/20\n",
      "500/500 [==============================] - 73s 145ms/step - loss: 0.2485 - val_loss: 0.2915\n",
      "Epoch 10/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2449 - val_loss: 0.2920\n",
      "Epoch 11/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2403 - val_loss: 0.2988\n",
      "Epoch 12/20\n",
      "500/500 [==============================] - 73s 145ms/step - loss: 0.2371 - val_loss: 0.2940\n",
      "Epoch 13/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2314 - val_loss: 0.2923\n",
      "Epoch 14/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2279 - val_loss: 0.3025\n",
      "Epoch 15/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2234 - val_loss: 0.2990\n",
      "Epoch 16/20\n",
      "500/500 [==============================] - 72s 145ms/step - loss: 0.2218 - val_loss: 0.2935\n",
      "Epoch 17/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2187 - val_loss: 0.3036\n",
      "Epoch 18/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2157 - val_loss: 0.2989\n",
      "Epoch 19/20\n",
      "500/500 [==============================] - 72s 144ms/step - loss: 0.2137 - val_loss: 0.3006\n",
      "Epoch 20/20\n",
      "248/500 [=============>................] - ETA: 26s - loss: 0.2103"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=RMSprop(), loss='mae')\n",
    "history = model.fit_generator(train_gen,\n",
    "                                      steps_per_epoch=500,\n",
    "                                      epochs=20,\n",
    "                                      validation_data=val_gen,\n",
    "                                      validation_steps=val_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from the validation loss, this setup isn’t as good as the regularized GRU alone, but it’s significantly faster. It looks at twice as much data, which in this case doesn’t appear to be hugely helpful but may be important for other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
