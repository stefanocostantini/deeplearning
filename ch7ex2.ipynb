{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using callbacks to act on a model during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A callback is an object (a class instance implementing specific methods) that is passed to the model in the call to fit and that is called by the model at various points during training. It has access to all the available data about the state of the model and its per- formance, and it can take action: interrupt training, save a model, load a different weight set, or otherwise alter the state of the model.\n",
    "Callbacks can be used for:\n",
    "* Model checkpointing: save current weights\n",
    "* Early stopping: interrupting when validation loss is no longer improving\n",
    "* Dynamically adjust parameters, such as the learning rate\n",
    "* Logging metrics during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now go through an example where we use the `Model Checkpoint` and `Early stopping` callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (not run)\n",
    "import keras\n",
    "\n",
    "# here we define the list of callbacks that need to be used during fit.\n",
    "# the first one would stop the training if the accuracy stops increasing\n",
    "# the second would monitor validation loss and save a new model every time this improves\n",
    "# (but would not overwrite the model if it does not)\n",
    "callback_list = [keras.callbacks.EarlyStopping(monitor='acc',patience=1),\n",
    "                 keras.callbacks.ModelCheckpoint(filepath='/home/ec2-user/models/callbacks/example1.h5',\n",
    "                                                 monitor='val_loss', save_best_only=True,)]\n",
    "\n",
    "model.compile(optmizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.fit(x,y, epochs=10, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example for the `ReduceLROnPlateau` callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (not run)\n",
    "# in this case the callback would reduce the learning rate (to 10% of its original value) \n",
    "# if it notices a plateau in validation loss for 10 epochs\n",
    "callbacks_list = [keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)]\n",
    "\n",
    "model.fit(x,y,epochs=100, batch_size=32, callbacks=callbacks_list, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to write your own callbacks. Callbacks are implemented by sub-classing the class `keras.callbacks.Callback`. (See book on page 251 for details).\n",
    "The following is an example of a custom callback that saves to disk (as Numpy arrays) the activations of every layer of the model at the end of every epoch, computed on the first sample of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (not run)\n",
    "import keras\n",
    "import numpy as np\n",
    "\n",
    "class ActivationLogger(keras.callbacks.Callback):\n",
    "    \n",
    "    #Called by the parent model before training, to inform the callback of what model will be calling it\n",
    "    def set_model(self, model):\n",
    "        self.model = model\n",
    "        layer_outputs = [layer.output for layer in model.layers]\n",
    "        self.activations_model = keras.models.Model(model.input, layer_outputs)\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.validation_data is None:\n",
    "            raise RuntimeError('Requires validation_data.')\n",
    "        validation_sample = self.validation_data[0][0:1]\n",
    "        activations = self.activations_model.predict(validation_sample)\n",
    "        f = open('activations_at_epoch_' + str(epoch) + '.npz', 'w')\n",
    "        np.savez(f, activations)\n",
    "        f.close()                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key purpose of TensorBoard is to help you visually monitor everything that goes on inside your model during training. If you’re monitoring more information than just the model’s final loss, you can develop a clearer vision of what the model does and doesn’t do, and you can make progress more quickly. TensorBoard gives you access to several neat features, all in your browser:\n",
    "* Visually monitoring metrics during training\n",
    "* Visualizing your model architecture\n",
    "* Visualizing histograms of activations and gradients\n",
    "* Exploring embeddings in 3D\n",
    "\n",
    "We demonstrate this by training a 1D convnet on the IMDB sentiment-analysis task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embed (Embedding)            (None, 500, 128)          256000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 494, 32)           28704     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 98, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 92, 32)            7200      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 291,937\n",
      "Trainable params: 291,937\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 2000\n",
    "# Maximum length of text to consider in each review (number of words)\n",
    "max_len = 500\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(max_features, 128,\n",
    "                           input_length=max_len,\n",
    "                           name='embed'))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.MaxPooling1D(5))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0894 - acc: 0.0328 - val_loss: 1.5641 - val_acc: 0.1368\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0942 - acc: 0.0340 - val_loss: 1.5579 - val_acc: 0.1276\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0898 - acc: 0.0314 - val_loss: 1.5985 - val_acc: 0.1254\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0908 - acc: 0.0305 - val_loss: 1.6225 - val_acc: 0.1206\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0898 - acc: 0.0251 - val_loss: 1.6049 - val_acc: 0.1170\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0950 - acc: 0.0271 - val_loss: 1.6112 - val_acc: 0.1186\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0866 - acc: 0.0209 - val_loss: 1.5729 - val_acc: 0.1200\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0883 - acc: 0.0223 - val_loss: 1.6184 - val_acc: 0.1228\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0907 - acc: 0.0272 - val_loss: 1.5698 - val_acc: 0.1120\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0904 - acc: 0.0278 - val_loss: 1.5639 - val_acc: 0.1170\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0887 - acc: 0.0252 - val_loss: 1.6965 - val_acc: 0.1134\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0890 - acc: 0.0223 - val_loss: 1.5654 - val_acc: 0.1164\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0892 - acc: 0.0238 - val_loss: 1.5743 - val_acc: 0.1138\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0885 - acc: 0.0169 - val_loss: 1.5875 - val_acc: 0.1070\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0892 - acc: 0.0192 - val_loss: 1.6145 - val_acc: 0.1054\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0867 - acc: 0.0187 - val_loss: 1.6488 - val_acc: 0.1062\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0860 - acc: 0.0154 - val_loss: 1.8196 - val_acc: 0.1128\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 63s 3ms/step - loss: 0.0915 - acc: 0.0209 - val_loss: 1.6478 - val_acc: 0.0968\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 62s 3ms/step - loss: 0.0861 - acc: 0.0192 - val_loss: 1.6429 - val_acc: 0.1034\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 61s 3ms/step - loss: 0.0889 - acc: 0.0159 - val_loss: 1.6894 - val_acc: 0.0934\n"
     ]
    }
   ],
   "source": [
    "# To use TensorBoard we need to create a directory where to save the output\n",
    "# And we need to use the appropriate callback.\n",
    "# This callback will record activation histograms and embeddings once every epoch\n",
    "callbacks = [keras.callbacks.TensorBoard(log_dir=\"/home/ec2-user/tensorboard\",\n",
    "                                         histogram_freq=1,\n",
    "                                         embeddings_freq=1)]\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    epochs=20,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2,\n",
    "                    callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
